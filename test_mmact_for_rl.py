from typing import List, Any, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import torch
from transformers import AutoTokenizer

from transformers.generation import TopKLogitsWarper

from RLinf.rlinf.models.embodiment.modules.value_head import ValueHead
from RLinf.rlinf.models.embodiment.model_utils import (
    compute_entropy_from_logits,
    compute_logprobs_from_logits,
)

from training.prompting_utils import UniversalPrompting
from models import MMACTModelLM, MAGVITv2
from training.utils import image_transform_tensor

class MMACTForRLActionPrediction(nn.Module):
    def __init__(
        self,
        model_path: str,
        vq_model_path: str,
        action_vocab_size=None,
        vocab_offset=None,
        device="cuda:0",
        timesteps=4,
        exec_steps=6,
        preprocessing_max_seq_length=1024,
        training_chunk_size=8,
        action_dim=16,
        robot_type: str = "franka",
        add_value_head: bool = True,
    ):  
        super().__init__()
        self.image_transform_tensor = image_transform_tensor
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, padding_side="left", local_files_only=True
        )
        self.device = device
        self.timesteps = timesteps
        self.exec_steps = exec_steps
        self.preprocessing_max_seq_length = preprocessing_max_seq_length
        self.training_chunk_size = training_chunk_size
        self.action_dim = action_dim
        self.model = MMACTModelLM.from_pretrained(
            model_path, torch_dtype=torch.bfloat16
        ).to(self.device)
        print("Finish loading checkpoint. Start loading vq-model.")
        self.vq_model = MAGVITv2.from_pretrained(vq_model_path).to(self.device)
        # self.vq_model.eval()
        # self.vq_model.requires_grad_(False)
        print("Finish loading vq-model.")
        self.vocab_offset = (
            vocab_offset
            if vocab_offset
            else self.model.config.vocab_size - self.model.config.action_vocab_size
        )
        self.action_vocab_size = (
            self.action_vocab_size
            if action_vocab_size
            else self.model.config.action_vocab_size
        )
        if robot_type == "franka":  # match training padding method
            max_action_prompt_len = (
                self.preprocessing_max_seq_length
                - self.training_chunk_size * (self.action_dim * 2)
                - 2
            )
        else:  # training total len - chunk_size * action_dim - <soa><eoa>
            max_action_prompt_len = (
                self.preprocessing_max_seq_length
                - self.training_chunk_size * self.action_dim
                - 2
            )

        self.uni_prompting = UniversalPrompting(
            self.tokenizer,
            special_tokens=(
                "<|soi|>",
                "<|eoi|>",
                "<|sov|>",
                "<|eov|>",
                "<|t2i|>",
                "<|mmu|>",
                "<|t2v|>",
                "<|v2v|>",
                "<|lvg|>",
                "<|mm2a|>",
                "<|soa|>",
                "<|eoa|>",
                "<|7dim|>",
                "<|14dim|>",
                "<|sostate|>",
                "<|eostate|>",
            ),
            ignore_id=-100,
            cond_dropout_prob=0.0,
            use_reserved_token=True,
            max_action_prompt_len=max_action_prompt_len,
        )
        if True:
            self.value_head = ValueHead(
                input_dim=4096,
                hidden_sizes=(512, 256, 128),
                output_dim=1,
                activation="relu",
                bias_last=True,
            )
            self.value_head.to(dtype=torch.bfloat16, device=self.device)

    def image_process_for_generate(self, images_list):
        """
        è¾“å…¥ images_list: [(16, 3, 256, 256), (16, 3, 256, 256)] (ä¸»è§†è§’å’Œæ‰‹è…•è§†è§’)
        è¿”å›: image_tokens ç»“æ„ä¸º [Batch_0_tokens, Batch_1_tokens, ..., Batch_15_tokens]
        æ¯ä¸ª Batch_i_tokens æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªè§†è§’ Token çš„åˆ—è¡¨ã€‚
        """
        # 1. åˆå§‹åŒ–ç»“æœåˆ—è¡¨ï¼Œé•¿åº¦ä¸º Batch Size (16)
        batch_size = images_list[0].shape[0]
        all_batch_tokens = [[] for _ in range(batch_size)]

        # 2. éå†è§†è§’ï¼ˆä¾‹å¦‚å…ˆå¤„ç†æ‰€æœ‰äººçš„ä¸»è§†è§’ï¼Œå†å¤„ç†æ‰€æœ‰äººçš„æ‰‹è…•è§†è§’ï¼‰
        for view_tensor in images_list:
            # view_tensor å½¢çŠ¶: (16, 3, 256, 256)
            view_tensor = view_tensor.to(self.device)
            target_dtype = next(self.vq_model.parameters()).dtype 
            view_tensor = view_tensor.to(dtype=target_dtype)
            with torch.no_grad():
                # åˆ©ç”¨ VQ-model çš„ Batch æ¨ç†ï¼Œä¸€æ¬¡æ€§å¤„ç† 16 å¼ å›¾
                # å‡è®¾ get_code è¿”å› (16, Token_Num)
                batch_tokens = self.vq_model.get_code(view_tensor) 
                
            # åŠ ä¸Šæ–‡æœ¬è¯è¡¨åç§»é‡
            batch_tokens = batch_tokens + len(self.uni_prompting.text_tokenizer)
            
            # 3. å°†ç»“æœåˆ†å‘åˆ°å¯¹åº”çš„æ ·æœ¬ä¸­
            # batch_tokens.cpu().unbind(0) å¾—åˆ° 16 ä¸ª token å‘é‡
            for i, single_img_tokens in enumerate(batch_tokens.cpu().unbind(0)):
                all_batch_tokens[i].append(single_img_tokens)

        return all_batch_tokens


    def quantize_state_with_offset(self, values, bins: int = 1024):
        """
        è¾“å…¥ values: Tensor å½¢çŠ¶ (16, 8)
        è¿”å›: List[Tensor], é•¿åº¦ 16, æ¯ä¸ª Tensor å½¢çŠ¶ (8,)
        """
        # 1. æ•´ä½“æˆªæ–­åˆ° [-1, 1]
        values = torch.clamp(values, min=-1.0, max=1.0)

        # 2. æ•´ä½“æ˜ å°„åˆ° [0, bins-1] å¹¶åŠ ä¸Š offset
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨å¯¹ (16, 8) é‡Œçš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œ
        indices = torch.round((values + 1) / 2 * (bins - 1)).long() + self.vocab_offset

        # 3. æ²¿ç»´åº¦ 0 (Batch ç»´åº¦) æ‹†åˆ†
        # unbind(0) ä¼šæŠŠ (16, 8) çš„ Tensor å˜æˆ 16 ä¸ª (8,) çš„ Tensor ç»„æˆçš„å…ƒç»„
        return list(indices.unbind(0))

    def dequantize_action_with_offset(
        self, action_tokens, bins: int = 1024
    ) -> torch.Tensor:
        action_tokens = action_tokens.to(torch.int).clamp(0, bins - 1)
        return (action_tokens / (bins - 1) * 2) - 1

    def input_process(self, inputs):
        images_tensor, text_tasks, state_tensor, prev_action_tokens = inputs
        batch_size = len(text_tasks)
        action_dim = [int(self.action_dim)] * batch_size
        prev_action_tokens = [prev_action_tokens + self.vocab_offset] * batch_size
        state_tokens = self.quantize_state_with_offset(state_tensor, bins=self.action_vocab_size)
        
        reshape_images_tensor = []
        for view in images_tensor:
            # å¯¹ä¸€ä¸ªè§†è§’ä¸‹çš„ 16 å¼ å›¾åˆ†åˆ«åš transform
            # unbind(0) å°† (16, 3, 256, 256) æ‹†æˆ 16 ä¸ª (3, 256, 256) çš„åˆ—è¡¨
            processed_batch = [self.image_transform_tensor(img) for img in view.unbind(0)]
            # é‡æ–°ç»„è£…å› Tensor: (16, 3, 256, 256)
            reshape_images_tensor.append(torch.stack(processed_batch))
        image_tokens = self.image_process_for_generate(reshape_images_tensor)
        input_ids, attention_masks, prompt_ids = self.uni_prompting(
            (
                image_tokens,
                text_tasks,
                state_tokens,
                prev_action_tokens,
                action_dim,
                self.device,
                self.training_chunk_size,
            ),
            "mm2a_gen",
        )
        # æ³¨æ„: è¿™é‡Œ prompt_ids[0] æ˜¯æ ‡é‡ï¼Œå› ä¸ºåšäº† left padding å¯¹é½
        return input_ids, attention_masks, prompt_ids[0]

    def get_actions(self, inputs):
        """
        Your inputs should include
        images_tensor(List,[head_image, wrist_image]), text_task, state_tensor,previous_action_tokens([] if not used in training)
        æ‰§è¡Œæ¨ç†å¹¶è¿”å›åŠ¨ä½œä»¥åŠç”¨äº RL è®¡ç®—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        input_ids, attention_masks, prompt_id = self.input_process(inputs)
        
        # [ä¿®æ”¹ç‚¹ 1] æ¥æ”¶ action_generate è¿”å›çš„ logits
        gen_token_ids, action_logits, prompt_hidden_states, step_map = self.model.action_generate(
            input_ids=input_ids,
            attention_mask=attention_masks,
            timesteps=self.timesteps,
            guidance_scale=0,
            chunk_size=self.training_chunk_size,
            action_dim=self.action_dim,
            prompt_id=prompt_id, # æ³¨æ„ï¼šä¸è¦åŠ æ‹¬å·
            uni_prompting=self.uni_prompting,
            temperature=0.0,
            action_vocab_size=self.action_vocab_size,
        )
        
        # gen_token_ids åº”è¯¥æ˜¯ 0-1023 çš„ bin index (ä¸å« offset)
        # åé‡åŒ–å¾—åˆ°ç‰©ç†åŠ¨ä½œ
        action_chunk = self.dequantize_action_with_offset(
            gen_token_ids, bins=self.action_vocab_size
        ).view(gen_token_ids.shape[0], self.training_chunk_size, self.action_dim)

        # [ä¿®æ”¹ç‚¹ 2] è¿”å›æ‰€æœ‰è®­ç»ƒéœ€è¦çš„ä¸Šä¸‹æ–‡æ•°æ®
        return (
            action_chunk,
            gen_token_ids.view(gen_token_ids.shape[0], self.training_chunk_size, self.action_dim),
            input_ids,
            attention_masks,
            prompt_id,
            action_logits,
            prompt_hidden_states,
            step_map,
        )
    
    def predict_action_batch(
            self,
            env_obs=None,
            calculate_values=True,  # æ–°å¢å‚æ•°: æ˜¯å¦è®¡ç®— Critic Value
            **kargs,
    ) -> tuple[np.ndarray, dict[str, Any]]:
        # 1. å‡†å¤‡è¾“å…¥æ•°æ®
        images_tensor = [env_obs["images"], env_obs["wrist_images"]]
        text_tasks = env_obs["task_descriptions"]
        state_tensor = env_obs["states"]
        state_tensor[:, 3:6] = state_tensor[:, 3:6] / 5.0  # quickly fix bug in state
        flat_prev_actions_tensors = torch.tensor([])

        # 2. æ‰§è¡Œæ¨ç†
        # [ä¿®æ”¹ 3] æ¥æ”¶ prompt_hidden_states
        (
            action_chunk, 
            token_ids, 
            prompt_input_ids, 
            prompt_attention_masks, 
            prompt_id, 
            action_logits, 
            prompt_hidden_states, # <--- è·å–å®ƒ
            step_map
        ) = self.get_actions(
            inputs=(
                images_tensor,
                text_tasks,
                state_tensor,
                flat_prev_actions_tensors,
            ),
        )

        # ================= Result è®¡ç®—é€»è¾‘ =================
        
        # 3. è®¡ç®— LogProbs
        # ---------------------------------------------------------------------
        # token_ids: [Batch, Chunk, Dim] -> Flatten [Batch, Chunk*Dim]
        batch_size = token_ids.shape[0]
        flat_action_ids = token_ids.reshape(batch_size, -1)
        
        # [ä¿®æ”¹ 4] ç›´æ¥è®¡ç®— LogProbs (ç§»é™¤ TopK å’Œ Temperature)
        # è¿™é‡Œçš„ action_logits å·²ç»æ˜¯ [Batch, Chunk*Dim, Action_Vocab_Size]
        action_logits = action_logits.permute(0, 2, 1)
        chunk_logprobs = compute_logprobs_from_logits(
            logits=action_logits, 
            target=flat_action_ids # æ³¨æ„å‚æ•°åé€šå¸¸æ˜¯ labels æˆ– targetï¼Œå–å†³äºä½ çš„ utils å®ç°
        ) 

        # 4. è®¡ç®— Values (State Value V(s))
        # ---------------------------------------------------------------------
        chunk_values = torch.zeros_like(chunk_logprobs[..., :1])

        # æ³¨æ„ï¼šè¿™é‡Œæ”¹ä¸ºæ£€æŸ¥ self.value_head (å› ä¸º FSDP é€‚é…æ—¶æˆ‘ä»¬å°†å®ƒæŒ‚åœ¨ self ä¸‹)
        if calculate_values and hasattr(self, "value_head"):
            # [ä¿®æ”¹ 5] æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒï¼šç›´æ¥å¤ç”¨ prompt_hidden_states
            # ä¸å†è¿›è¡Œé¢å¤–çš„ forward passï¼ŒèŠ‚çœ 50% æ˜¾å­˜å’Œè®¡ç®—
            
            # prompt_hidden_states: [Batch, Seq_Len, Hidden_Dim]
            # æˆ‘ä»¬æå– <soa> ä½ç½®çš„ç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯ prompt_id æ‰€åœ¨çš„ä½ç½®
            
            # ç¡®ä¿åˆ‡ç‰‡å‡ºæ¥çš„ tensor ä¸éœ€è¦æ¢¯åº¦ï¼ˆCritic åªéœ€è¦è¾“å…¥ç‰¹å¾ï¼‰
            # è™½ç„¶ inference æ¨¡å¼ä¸‹é€šå¸¸è‡ªå¸¦ no_gradï¼Œä½† detach æ›´ä¿é™©
            state_hidden = prompt_hidden_states[:, prompt_id, :]
            
            # å¿…é¡»ç¡®ä¿è¾“å…¥ç²¾åº¦æ˜¯ bfloat16 (å› ä¸º value_head å·²è½¬ä¸º bf16)
            if state_hidden.dtype != torch.bfloat16:
                 state_hidden = state_hidden.to(dtype=torch.bfloat16)

            chunk_values = self.value_head(state_hidden)

        # 5. æ„é€  Forward Inputs
        # ---------------------------------------------------------------------
        full_input_ids = prompt_input_ids.clone()
        
        num_action_tokens = flat_action_ids.shape[1]
        action_slice = slice(prompt_id + 1, prompt_id + 1 + num_action_tokens)
        
        full_input_ids[:, action_slice] = flat_action_ids + self.vocab_offset

        forward_inputs = {
            "input_ids": full_input_ids.cpu(),
            "attention_mask": prompt_attention_masks.cpu(),
            "action_tokens": token_ids.cpu(),
        }

        result = {
            "prev_logprobs": chunk_logprobs,
            "prev_values": chunk_values,
            "step_map": step_map,
            "forward_inputs": forward_inputs,
        }
        
        # [ä¿®æ”¹ 6] è¿”å› NumPy æ•°ç»„ç»™ EnvWorker
        return action_chunk.float().cpu().numpy(), result
    
    def forward(
        self,
        data: dict[str, torch.Tensor],
        compute_logprobs: bool = True,
        compute_entropy: bool = False,
        compute_values: bool = False,
        use_cache: bool = False,
    ):
        """
        PPO Training Forward Pass (Teacher Forcing)
        """
        # 1. è§£åŒ…æ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        # input_ids å·²ç»åŒ…å«äº† [Prompt ... <soa> ... Actions]
        input_ids = data["input_ids"].to(self.device)
        attention_mask = data["attention_mask"].to(self.device)
        # target_action_tokens æ˜¯ç›¸å¯¹ç´¢å¼• (0-1023)ï¼Œç”¨äºè®¡ç®— Loss
        target_action_tokens = data["action_tokens"].to(self.device)

        # 2. æ¨¡å‹å‰å‘ä¼ æ’­ (Parallel)
        # å¦‚æœéœ€è¦è®¡ç®— Valueï¼Œå¿…é¡»å¼€å¯ output_hidden_states
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=compute_values,
            use_cache=use_cache,
            return_dict=True
        )

        output_dict = {}

        # 3. è®¡ç®—å…³é”®ä½ç½®ç´¢å¼•
        # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŠ¨ä½œå¼€å§‹çš„ä½ç½® (<soa>)
        # input_ids å¸ƒå±€: [Prompt_Tokens ... <soa>, Act_1, Act_2, ..., Act_N]
        # åŠ¨ä½œæ€»é•¿åº¦ = chunk_size * action_dim
        num_action_tokens = self.training_chunk_size * self.action_dim
        
        # <soa> çš„ç´¢å¼•ä½ç½® = æ€»é•¿åº¦ - åŠ¨ä½œé•¿åº¦ - 1
        prompt_end_idx = input_ids.shape[1] - num_action_tokens - 1

        # 4. è®¡ç®— Values (Critic)
        # åªéœ€è¦ <soa> ä½ç½®çš„ hidden stateï¼Œä»£è¡¨åŠ¨ä½œæ‰§è¡Œå‰çš„çŠ¶æ€ V(s)
        if compute_values and hasattr(self, "value_head"):
            # å–æœ€åä¸€å±‚ hidden state
            # Shape: [Batch, Hidden_Dim]
            soa_hidden = outputs.hidden_states[-1][:, prompt_end_idx, :]
            
            # ç²¾åº¦å¯¹é½ (ValueHead æ˜¯ bfloat16)
            if soa_hidden.dtype != torch.bfloat16:
                soa_hidden = soa_hidden.to(dtype=torch.bfloat16)
                
            output_dict["values"] = self.value_head(soa_hidden) # [B, 1]

        # 5. è®¡ç®— LogProbs & Entropy (Actor)
        if compute_logprobs or compute_entropy:
            # é€»è¾‘ï¼šä½ç½® i çš„ Logits é¢„æµ‹ä½ç½® i+1 çš„ Token
            # æˆ‘ä»¬éœ€è¦é¢„æµ‹: Act_1, Act_2, ..., Act_N
            # å¯¹åº”çš„è¾“å…¥ä½ç½®æ˜¯: <soa>, Act_1, ..., Act_{N-1}
            # æ‰€ä»¥åˆ‡ç‰‡èŒƒå›´æ˜¯: [prompt_end_idx : -1]
            
            # [Batch, Num_Action_Tokens, Vocab_Size]
            relevant_logits = outputs.logits[:, prompt_end_idx:-1, :]

            # è¯è¡¨åˆ‡ç‰‡ï¼šåªä¿ç•™ Action Bins éƒ¨åˆ†
            # [Batch, Num_Action_Tokens, Action_Vocab_Size]
            action_logits = relevant_logits[..., self.vocab_offset : self.vocab_offset + self.action_vocab_size]

            # ç»´åº¦è½¬ç½® (é€‚é… CrossEntropy): [B, Seq, Vocab] -> [B, Vocab, Seq]
            action_logits = action_logits.permute(0, 2, 1)

            if compute_logprobs:
                # å±•å¹³ Target ä»¥åŒ¹é… Logits ç»´åº¦
                flat_targets = target_action_tokens.reshape(input_ids.shape[0], -1)
                
                output_dict["logprobs"] = compute_logprobs_from_logits(
                    logits=action_logits,
                    target=flat_targets
                )

            if compute_entropy:
                output_dict["entropy"] = compute_entropy_from_logits(
                    logits=action_logits
                )

        return output_dict
    
if __name__ == "__main__":
    # ================= é…ç½®åŒºåŸŸ =================
    # è¯·å°†ä»¥ä¸‹è·¯å¾„ä¿®æ”¹ä¸ºä½ æœåŠ¡å™¨ä¸Šå®é™…çš„æ¨¡å‹è·¯å¾„
    MODEL_PATH = "/mnt/pfs/scalelab2/ch/MM-ACT/checkpoints/MM-ACT-10" 
    VQ_MODEL_PATH = "/mnt/pfs/scalelab2/yitian-proj/MM-ACT/huggingface/hub/models--showlab--magvitv2/snapshots/5c3fa78f8b3523347c5cd1a4c97f3c4e96f33d5d"
    
    # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æ”¯æŒ CUDA
    DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
    # ===========================================

    print(f"ğŸš€ [Test] æ­£åœ¨åˆå§‹åŒ– MMACTForRLActionPrediction (Device: {DEVICE})...")
    
    # 1. å®ä¾‹åŒ–æ¨¡å‹
    # æ³¨æ„ï¼šæ ¹æ®ä½ çš„ states shape (16, 8)ï¼Œè¿™é‡Œçš„ action_dim åº”è¯¥è®¾ä¸º 8 ä»¥åŒ¹é…
    try:
        model = MMACTForRLActionPrediction(
            model_path=MODEL_PATH,
            vq_model_path=VQ_MODEL_PATH,
            device=DEVICE,
            action_dim=7,
            robot_type="franka",
            training_chunk_size=8,
        )
    except OSError as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®:\n{e}")
        sys.exit(1)

    # 2. æ„é€ æ¨¡æ‹Ÿè¾“å…¥æ•°æ® (Batch Size = 16)
    BATCH_SIZE = 16
    print(f"ğŸ“¦ [Test] æ­£åœ¨æ„é€  Batch Size = {BATCH_SIZE} çš„æ¨¡æ‹Ÿ env_obs æ•°æ®...")

    # æ¨¡æ‹Ÿ uint8 ç±»å‹çš„åŸå§‹å›¾åƒ (0-255)
    # å½¢çŠ¶: (Batch, Channel, Height, Width)
    dummy_images = torch.randint(0, 256, (BATCH_SIZE, 3, 256, 256), dtype=torch.uint8)
    dummy_wrist_images = torch.randint(0, 256, (BATCH_SIZE, 3, 256, 256), dtype=torch.uint8)
    
    # æ¨¡æ‹Ÿ float32 ç±»å‹çš„çŠ¶æ€å‘é‡
    # å½¢çŠ¶: (Batch, State_Dim) -> (16, 8)
    dummy_states = torch.randn((BATCH_SIZE, 8), dtype=torch.float32)

    # æ¨¡æ‹Ÿä»»åŠ¡æè¿°åˆ—è¡¨
    dummy_tasks = [
        "put the white mug on the plate and put the chocolate"
    ] * BATCH_SIZE

    env_obs = {
        "images": dummy_images,
        "wrist_images": dummy_wrist_images,
        "states": dummy_states,
        "task_descriptions": dummy_tasks
    }

    # 3. æ‰“å°è¾“å…¥ä¿¡æ¯ä»¥ä¾¿æ ¸å¯¹
    print("\nğŸ“‹ è¾“å…¥æ•°æ®æ¦‚è§ˆ:")
    for k, v in env_obs.items():
        if isinstance(v, torch.Tensor):
            print(f"  - {k}: Tensor {tuple(v.shape)} | {v.dtype}")
        elif isinstance(v, list):
            print(f"  - {k}: List len={len(v)} | First: {v[0][:20]}...")

    # 4. æ‰§è¡Œé¢„æµ‹
    print("\nâš¡ï¸ [Test] å¼€å§‹æ‰§è¡Œ predict_action_batch ...")
    try:
        # è¿™ä¸€æ­¥ä¼šæµ‹è¯•:
        # 1. state å½’ä¸€åŒ– (x/5.0)
        # 2. å›¾åƒ Batch é¢„å¤„ç† (uint8 -> float -> norm)
        # 3. VQ-Model Batch æ¨ç†
        # 4. Prompt ç»„è£…ä¸ LLM ç”Ÿæˆ
        action_chunk, result = model.predict_action_batch(env_obs)
        
        print("\nâœ… æµ‹è¯•æˆåŠŸ! è¾“å‡ºç»“æœ:")
        print("=" * 40)
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        print(f"Output Action Chunk Shape: {action_chunk.shape}") 
        
        # é¢„æœŸå½¢çŠ¶é€šå¸¸æ˜¯ (chunk_size, action_dim) æˆ–è€… (Batch, chunk_size, action_dim)
        # å–å†³äºä½ çš„ get_actions å®ç°è¿”å›çš„æ˜¯å•æ¡è¿˜æ˜¯ Batch
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´ (å½’ä¸€åŒ–åçš„åŠ¨ä½œé€šå¸¸åœ¨ -1 åˆ° 1 ä¹‹é—´)
        print(f"Action Values Range:       [{action_chunk.min().item():.3f}, {action_chunk.max().item():.3f}]")
        print("=" * 40)

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()