experiment:
  project: "rl_mmact_libero_action"  # need to be same of this file name
  name: ""
  output_dir: ""
  function: "train"   # TODO: add eval function later
  current_epoch: 1
  deepspeed_file: "TODO"
  num_node: 1
  node_index: 0  # no need to change

model:
  mmact:
    pretrained_model_path: "/mnt/pfs/scalelab2/yitian-proj/MM-ACT/huggingface/mmada-modified"
    optimized_name: "optimized"

  vq_model:
    type: "magvitv2"
    vq_model_name: "/mnt/pfs/scalelab2/yitian-proj/MM-ACT/huggingface/hub/models--showlab--magvitv2/snapshots/5c3fa78f8b3523347c5cd1a4c97f3c4e96f33d5d"

dataset:
  preprocessing:
    max_seq_length: 1024
  dataset_file_paths: 
    - "/mnt/pfs/scalelab2/yitian-proj/MM-ACT/huggingface/hub/datasets--lerobot--libero_10_image/snapshots/9b902906e722dbb26c29e89c6b45cd77f743d87a"
  data_type: "libero_action"

training:
  batch_size: 2
  num_epochs: 200 #actually use about 20epoch
  chunk_size: 8
  noise_type: "mask"
  log_interval: 50
  predict_all_tokens: True #set True if predict all action tokens in one step
  # max_train_steps: 1000000
  dataprocess_nums: 16
  action_loss_type: "single"
  use_prev_action: False


optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 5e-5
        scale_lr: False
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 500
        min_lr_scale: 0.2

rollout:
    num_task_per_step: 32
    num_response_per_task: 8
    temperature: 0.8
    steps: 512
    max_gen_length: 512
    batch_size: 2
    remasking_strategy: "low_confidence_static" #"low_confidence_static""low_confidence_dynamic"
    target: "confidence" # target to decide which tokens to unmask, eg. confidence, margin_confidence and neg_entropy
    dynamic_threshold: 0.95 # no use for "low_confidence_static"
    block_size: 32
    further_horizon: 128
    use_cache: True   # TODO: 暂时没有实现True的逻辑